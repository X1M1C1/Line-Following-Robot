<!--
Gautham Ponnu
Template for
-->

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<html>
<head>

	<title>LineZoomer </title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />

	<meta name="keywords" content="Lawrence Atienza, Xavier Mc Court, line-following/graph traversing robot" />
	<meta name="description" content="This webpage is our project report for ECE 5725 at Cornell University" />
	<link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/2.7.0/build/reset-fonts-grids/reset-fonts-grids.css" media="all" />
	<link rel="stylesheet" type="text/css" href="CSS/report.css" media="all" />
	<link rel="stylesheet" href="CSS/project.css" type="text/css" media="screen, projection" />
</head>
<body>

<div id="doc2" class="yui-t7">
	<div id="inner">
		<div id="hd">
			<div class="yui-gc">
				<div class="yui-u first">
					<h1>The LineZoomer</h1>
					<h2>ECE 5725 Fall 2024 project </h2>
					<h3>by Lawrience Atienza (laa236), Xavier Mc Court (xmm3) </h3>
				</div>

			</div><!--// .yui-gc -->
		</div><!--// hd -->

		<div id="bd"> <!--// Main Body begins here -->
			<div id="yui-main">
				<div class="yui-b">


						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Introduction</h2> 
							</div>
							<div class="yui-u">
								<p>Our project was to build an autonomous, grid traversing robot, capable of reacting to situations like a blocked road and always taking the most optimal path. We aimed to do this by implementing a mix of line following, intersection detection and object detection on a Rpi based wheeled robot. Hence the LineZoomer was born. 
								</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is another section -->
							<div class="yui-u first">
								<h2>Project Objectives:</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>
									-be able to detect a line
									-be able to detect an intersection
									-be able to detect an object in the robot's path and react to it properly (no collision)
									-be able to precisely control the robot's trajectory based on real time inputs from it's sensors 
									-make the robot relisilient to various lighting conditions and environments
									-ensure the robot ends gracefully / abandons it's mission when edge case solutionless solutions, avoid any unwanted behavior
									-make the robot completely autonomous (once inputed with a grid geometry, his point of origin and his destination) and launched
									-bring the previous points together to create the LineZoomer			
									-learn more about embedded and have fun!

								</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Design</h2> <!--// put your heading here -->
								<h2>Software Design</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Our implementation of the LineZoomer works by line following until it detects an intersection, it then looks at the turning commands given by it's algorithm in order to decide where it needs to turn next once it has reached an intersection. 
									We will start by detailing how it determines the path it needs to follow.
									Our software start by generating a rectangular graph of dimensions n*m. It is entirely represented by it's agency list, in other owrds as a (nm,nm) matrix G wjer the Gi,j coefficient represents the distance between the directional path going from node i to node j. Hence in the case of our project our matrix is symmetrical and all coefficients being set to +inifinty (infinite distance meaning that there is no path between the two nodes) and coefficients Gi,i+1 Gi,i-1 Gi,i-m Gi,i+m  being set to 1. 
									We then select an starting point and a destination for our robot out of the various nodes of our rectangular graph and use Dijkstra's algorithm to find the optimal path between these nodes.
									Dijkstra's algorithm works along the lines of the foollowing pseudo code: 
								function Dijkstra(graph, source):
									distance = [infinity] * len(graph)
									distance[source] = 0
									priority_queue = [(0, source)]  # (distance, node)
									
									while priority_queue is not empty:
										current_distance, current_node = priority_queue.pop_min()
										
										if current_distance > distance[current_node]:
											continue  # Skip if already finalized
										
										for neighbor, weight in graph[current_node]:
											distance_via_current = current_distance + weight
											
											if distance_via_current < distance[neighbor]:
												distance[neighbor] = distance_via_current
												priority_queue.add((distance_via_current, neighbor))
									
									return distance
									We coded the algorithm with a priority queue implemented as a binary heap resulting in an algorithm complexity of O((V+E)logV) with VV = n*m: Number of vertices and V = 2n*m: Number of edges.
									In order to handle the case when the robot encouters an obstacle on the optimal bath it wants to take, we simply update the distance value of the given edge to + infinity in our graph representation, return to the previous node and re-calculate a new optimal path by calling Dijkstra's algorithm with our current node as new starting point and the same destination as before. 
									Our current algorithm returns the path the robot as a list of nodes he has to go to in order to reach the destination optimally. However our robot takes as an input durning directions such a "left", "right", "forward", "backward" hence we have to write a conversion algorithm. This algorithm uses the cardinality of the next node ("North", "East", "South", "West" and the current orientation of the robot that is deduced from the turns performed up to now and the initial orientation of the robot with regards to the grid that is an input variable, in order to determine the next relative direction the robot has to turn in).
									We also wrote a code that gives a visual representation of the robot as he traverses the algorithm. It takes into the account the starting point of the robot, it's current position,  the destination, any objects found during exploration and the various paths that have been tried in the past as well as the current one it is following. 

									We will now detail how it line follow and detects intersections. 
									Our robot uses two separate image processing functions in order to adequately line follow.
									We start by processing the image and identifying the black lines to follow in it, this is done by many steps that vary depending on which of the following algorithms are used, but they generally involve cropping the iamge to an adapted field of view to remove any far away distractions and shawing due to the angles, blurring it to remove details ( ameasure to  try and remove small shadows, texutre of the ground, texture of the tape that forms the black line, small halos of light reflecting of the ground or tape,...), grayscaling it, before applying a binary mask to separate what should be the black line from the background. We then apply some fine tuned  kernel size erosion and dilatation to try and further remove noise in the backa nd white sections of the binary masked iamge.  
									 We the plot the straight line that best fitst the aforemetnioned band. When it off center from the line (aka the line does not intersect the bottom of the central third of the robot's field of vision) ,not aligned at all, or simply doesn't detect any clear black line it uses our first following pardigm.
									  It detects the biggest black shape image and aims towards the barycenter of black that is visible, this ensure the robot keeps going forwards all while it turn towards the line, getting it back into alignement with the line. 
									  On the other hand when the robot is already pretty well aligned with the line it simply tries to line up as best as possible with it and keep heading forwards whilst staying on trap.

									When it comes to intersection detection we simply foind countour of the varuous black shapes on the iamge and study the complexity (number of edges) of the shape. We apply some filtering such as a minimal aspect ration between the size od the edges and the shape in order to stop shadow or blurring to add small non linearities to the shape that would count towards it's nmber of edges. When a shape of more than 5 edges is detected on the iamge, the program sends back an intersection detected glag to the robot. This works because all junctions we to deal with ahve at least 6 edges (Corner), 8 edges (T-junction) and  edhes (cross intersection).
									
								</p> <!--// Put Content here -->
								<!--// Put Content here, Put the FSM model here  --> 
							</div>
							<div class="yui-u first">
								<h2>Hardware Design</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Our robot uses several sensors to gather data, and servos to interact with the world.</p>
									<p>For line detection, we use a Raspberry Pi Camera module mounted with rubber bands on two popsticle sticks. While the sticks are hot glued on, the rubber bands allow for the camera to be adjusted. We use folded sticky notes to aim the camera downwards towards the grid.
									Our line detection algorithm is defined above, but our robot only needs the detected line angle for line following. It simply angles back towards the line if the detected angle is larger than +/-30 degrees. If the robot is with this band, it simply moves forwards a bit.
									In order to protect the robot from driving into the void if no line is spotted, we save the most recent turn direction. If we cannot see a line, we simply turn in the reverse direction until the line is found. This helps not only during normal line following operation, but also when we overshoot at 90 degree turn. In fact, it is easier to recover from an overshoot than an undershoot, so we intentionally overshoot the 90 turn. 
									We move turn in short bursts in order to prevent the robot from getting stuck "wiggling". If the robot was allowed to constantly turn while orienting itself towards the line, it almost always repeatedly misses the center, due to the robot turning faster than it can process the line detection.
									We also nudge the robot forward slightly when doing a turn. We found this helps it get back on track, by preventing the robot from getting stuck in a wiggle.</p>

									<p>For object detection, we use an ultrasonic sensor aimed forwards, hanging below the main chassis. We do not secure it other than the connecting wires. While this is not a great idea, it is very lightweight, and has no force exerted on it, while not needing to be precisely aimed.
									This is simply called once every second or so, and its output has priority in the FSM to determine the next robot stage.</p>
									
									<p>For movement, we used two Parallax servos. These are controlled via PWM, and can report their current angle via an output PWM. 
									We had to drill new holes in the chassis to accommodate then new servo mountings, and this resulted in an imperfect alignment. Our robot tends to veer to the left when it goes straight for a period of time. Thus, we rarely move for a long period of time, and instead move in short bursts to error correct. 
									The Hall effect sensors which find the servo angle is also slightly inaccurate, and moving in shorter bursts helps mitigate issues that appear in longer runs.
									Instead of using time to determine when the robot turns 90 degrees, or when it moves a certain distance, we use the rotation angle reported by the Hall sensors. This is more accurate than using time, as time depends on the servo strength and the traverse material.</p>

									<p>We also used a PiTFT. We had some ideas to use to display / control the current robot state, but we ran out of time. It is currently used only to show the LAN IP of the robot, in order to SSH to the Pi.</p>
								
									<p>We power the Pi (and PiTFT, camera, and ultrasonic sensor) using a 5V rechargable battery pack. As we changed the robot around a bunch of times, we did not mount the battery pack, and kept it leashed with a long USB cable. The servos are powered using 4x AA batteries in series, as they need a higher voltage.</p>
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Testing</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>We planned and applied a rigourous testing strategy during our project, ensuring that every previous part of our project was fully functional before attempting to implment "the next thing":
									We started by ensuring our various components were functional and worked with our RPi. we checked oir control over the motor rotation and servomotor pwm signal receival. We then ensured we could receive the expected signal from the ultrasound sensor. 
								Once the robot was build and an MVP version of the code written we started testing our robot on small grids and interesction on the ground. These first tests led us to make thing easier on ourselves by aiming to use similar lighting conditions each time (if possible multiple light sources all aournd the robot to remove as many shadows as possible, making our grid on some taped together white sheets to ensure a smoothed, not too textured  and perfectly white surface wihout any designs on it.) These tests also lead us to fesign beigger grids for our following testing, as on smaller girds the intersection detection part of the algorithm always be active as the robot would almost always see the nest intersection when he was still on the previous node, this means that despite the robot completing it's task it wasn't making use of it's line folllowing capabilities. 
							We thouroughly tested each new feature we added to our MVP as we tried to improve it's initally low speed and aforementioned guidance problems. When we had settled in a satisfactory final grid and final performance state of the robot we performed a battery of tests to ensure that it was working as expected. We tested positions of the staring, and ending points as well as various objects on the grid. We also did some edge case testing, to ensure that the robot behaved safely when for example  all edges leading to it's target node were blocked by an obstacle, leadinf it to give up and stop, or simply when it's starting and ending positions were the same. Once these tests were successfully pasted we performed some more testing by varying the rooms we set up our graph in order to observe it's performance in various different lighting conditions. We were surprised as our final code was a lot more resilient than we initially expected, leaps and bounds above our inital mvp performance as our robot succeded all of the reasonable lighting tests we gave it. Even in rooms with rather directional lighting compared to the robot's positon, hence creating shadows in our robot's line of sight our robot manged to reach it's destination safely, this was a far cry from our inital tests where we had to hold a phone lamp by hand above the camera of our robot in order to remove and shadows in that area. 
													</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Drawings</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
								</p> <br /> <p> This is an example image  </p> <!--// Put Content here -->
								<img src="images/incident.png" title="Ha Ha ... that's me" alt="This is a picture" width="420" height="200"/> </p>
								<p> Credits to XKCD </p>
							</div>
						</div><!--// Your section has ended -->
						
						<div class="yui-gf"> <!--// This is a section -->
						    <div class="yui-u first">
						        <h2>Limitations</h2> <!--// Put your heading here -->
						    </div>
						    <div class="yui-u">
						        <p>While we managed to fix most problems that appeared, there were some issues that we could not handle.</p>
						
						        <h3>Camera</h3>
						        <ul>
						            <li>The camera has a small FOV, and thus it can be difficult to acquire the line.</li>
						            <li>The robot does not handle small grids well, as it cannot see closer than ~5.5 in away from the axles. Grids of a smaller shape cannot be traversed properly.</li>
						        </ul>
						
						        <h3>Detection</h3>
						        <ul>
						            <li>The filters we used are resilient, but have issues when the environment has bad lighting. We managed to make it resilient enough to not need perfect lighting, but some issues still remain.</li>
						            <li>The filters require a dark line on a light grid.</li>
						            <li>The robot cannot detect chasms, only obstacles.</li>
						            <li>In certain scenarios when the robot rapidly switches between the barycenter and line center detection algorithms, wiggling may occur. The problem eventually resolves itself, as the robot is nudged forward on every wiggle, but it drastically slows down movement.</li>
						            <li>Lines which are not simple rectangles and are more complex (have more edges), can be falsely identified as intersections.</li>
						            <li>The robot needs flat terrain (with decent traction). Turning on varied terrain yields inconsistent end angles, and terrain shadows may interfere with detection.</li>
						        </ul>
						
						        <h3>Movement</h3>
						        <ul>
						            <li>While the robot can follow gently-curved paths and irregular graphs, it can only deal with intersections at 90 degrees.</li>
						            <li>The robot also has a center of rotation in the front, and thus requires more space to turn than would be expected, being another reason why small grids do not work.</li>
						            <li>The robot turns a little when going straight a long distance due to non-perfect wheel alignment. We mitigate this by not driving in a straight line for long periods of time.</li>
						            <li>When turns start at very specific angles which result in a destination angle near 360 (or 0) degrees, the instability of the Hall sensors can result in the robot not moving far enough (terminating move command early).</li>
						            <li>Applying any external rotation to the wheels during precise forward movement or precise turning can cause incorrect movement.</li>
						        </ul>
						
						        <h3>Ultrasonic Sensor</h3>
						        <ul>
						            <li>The ultrasonic sensor may not detect objects that would hit the top of the robot, due to its position on the lower half of the chassis.</li>
						        </ul>
						
						        <h3>Controls / Pathfinding</h3>
						        <ul>
						            <li>The robot has no controls on itself, and can only be controlled via SSH.</li>
						            <li>The robot is leashed to a power pack, which must be moved when the robot moves far enough away.</li>
						            <li>The edge case of having the start node be the same as the end node is unhandled.</li>
						        </ul>
						    </div>
						</div><!--// Your section has ended -->


						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Results</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
								</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Conclusion</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p> In short, except from installing our initially planned self targetting a nerf-dart launching system that would have made the LineZoomer into the LineZoomingTank, we managed to obtain all the functioanlities we expected out of our robot, it even turned out to be more resilient than expected. WIth a near 100% success rate in all the tests we performed, even in difficult lighting coditions.
									Future impvoement to our LineZOomer would probably targetted at improving it's speed as it is currently an overly cautious robot that refuses to go forwards much until it's pefectly aligned with the line. This often leadint it to perform many- time consuming micro adjustments before any decisive push forward. With more time we would also have like to improve the user interface, for example by displaying our visual represenation of the graph, the paths attempted, the position of hte obstaclesa nd the current position of the robot on the Pitft screen in real time. We would also have liked put hte starting buttong for out robot on the Pitft screen. With more time we could have tried to implment out tank upgrade, where our robot once hacing reached it's final destination woudld start turning on itself, as a kind of centry turret, in order to find (with it's camera and maybe some face recognition) and shoot a nerf pellet at the neighbouring target. 							</p> 
									<!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Work Distribution</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p> Lawrence took care of the most of the hardware testing and setup as well as the actual robot's "brain", whilst Xavier handled all of the iamge processing, graph creation, optimal path calculation and translation as well as the step by step visualization of the robot's position. We both equally participated in the debugging and testing of each iteration of our robot, proposing out ideas on how to improve it even on parts that we didn't directly code. 									<!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->
						
						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Git hub link:</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p> All our code is open source, feel free to use it to tryan make your own robot our improve our current one! Here's a link to the github repo we created: 									<!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

					<div class="yui-gf">
						<div class="yui-u first">
							<h2>Code Appendix</h2>
						</div>
							<div class="yui-u">
							<!-- This is a good idea. HTML generated using hilite.me -->
							<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td><pre style="margin: 0; line-height: 125%"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #333333">*</span>
 <span style="color: #333333">*</span> blink.c<span style="color: #333333">:</span>
 <span style="color: #333333">*</span>      blinks the first LED
 <span style="color: #333333">*</span>      Gordon Henderson, projects<span style="color: #FF0000; background-color: #FFAAAA">@</span>drogon.net
 <span style="color: #FF0000; background-color: #FFAAAA">*/</span>

<span style="color: #557799">#include &lt;stdio.h&gt;</span>
<span style="color: #557799">#include &lt;wiringPi.h&gt;</span>

<span style="color: #333399; font-weight: bold">int</span> main (<span style="color: #333399; font-weight: bold">void</span>)
{
  printf (<span style="background-color: #fff0f0">&quot;Raspberry Pi blink</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&quot;</span>) ;

  <span style="color: #008800; font-weight: bold">if</span> (wiringPiSetup () <span style="color: #333333">==</span> <span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">1</span>)
    <span style="color: #008800; font-weight: bold">return</span> <span style="color: #0000DD; font-weight: bold">1</span> ;

  pinMode (<span style="color: #0000DD; font-weight: bold">0</span>, OUTPUT) ;         <span style="color: #888888">// aka BCM_GPIO pin 17</span>

  <span style="color: #008800; font-weight: bold">for</span> (;;)
  {
    digitalWrite (<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">1</span>) ;       <span style="color: #888888">// On</span>
    delay (<span style="color: #0000DD; font-weight: bold">500</span>) ;               <span style="color: #888888">// mS</span>
    digitalWrite (<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">0</span>) ;       <span style="color: #888888">// Off</span>
    delay (<span style="color: #0000DD; font-weight: bold">500</span>) ;
  }
  <span style="color: #008800; font-weight: bold">return</span> <span style="color: #0000DD; font-weight: bold">0</span> ;
}
</pre></td></tr></table></div>

						</div>
					</div>
					</div>

					<div class="yui-gf">
						<div class="yui-u first">
							<h2>Contact</h2>
						</div>
						<div class="yui-u">
						<p> Your good names. Probably Acknowledgments and Thanks </p>
						</div>
					</div>
					</div>

				</div><!--// .yui-b -->
			</div><!--// yui-main -->
		</div><!--// bd -->

		<div id="ft">
			<p><a href="http://validator.w3.org/check?uri=referer"><img src=
			"images/w3chatered.gif" title="W3C+Hates+Me" alt="W3C+Hates+Me"
			width="80" height="15" /></a>
			<a href="http://jigsaw.w3.org/css-validator/check/referer"><img src=
			"images/css_copy.gif" title="Valid+CSS%21" alt="Valid+CSS%21"
			width="80" height="15" /></a>
			<img src="images/code.gif" title="Handcrafted with sweat and blood" alt="Handcrafted with sweat and blood" width="80" height="15" />
			<img src="images/anybrowser.gif" title="Run Any Browser Any OS" alt="Runs on Any Browser Any OS" width="80" height="15" /></p>
			<br />
		</div><!--// footer -->

	</div><!-- // inner -->


</div><!--// doc -->


</body>
</html>
