<!--
Gautham Ponnu
Template for
-->

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<html>
<head>

	<title>LineZoomer </title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />

	<meta name="keywords" content="Lawrence Atienza, Xavier Mc Court, line-following/graph traversing robot" />
	<meta name="description" content="This webpage is our project report for ECE 5725 at Cornell University" />
	<link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/2.7.0/build/reset-fonts-grids/reset-fonts-grids.css" media="all" />
	<link rel="stylesheet" type="text/css" href="CSS/report.css" media="all" />
	<link rel="stylesheet" href="CSS/project.css" type="text/css" media="screen, projection" />
</head>
<body>

<div id="doc2" class="yui-t7">
	<div id="inner">
		<div id="hd">
			<div class="yui-gc">
				<div class="yui-u first">
					<h1>The LineZoomer</h1>
					<h2>ECE 5725 Fall 2024 project </h2>
					<h3>by Lawrience Atienza (laa236), Xavier Mc Court (xmm3) </h3>
				</div>

			</div><!--// .yui-gc -->
		</div><!--// hd -->

		<div id="bd"> <!--// Main Body begins here -->
			<div id="yui-main">
				<div class="yui-b">


						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Introduction</h2> 
							</div>
							<div class="yui-u">
								<p>Our project was to build an autonomous, grid traversing robot, capable of reacting to situations like a blocked road and always taking the most optimal path. We aimed to do this by implementing a mix of line following, intersection detection and object detection on a Rpi based wheeled robot. Hence the LineZoomer was born. 
								</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is another section -->
							<div class="yui-u first">
								<h2>Project Objectives:</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
							    <ul>
							        <li>Be able to detect a line.</li>
							        <li>Be able to detect an intersection.</li>
							        <li>Be able to detect an object in the robot's path and react to it properly (avoid collisions).</li>
							        <li>Be able to precisely control the robot's trajectory based on real-time inputs from its sensors.</li>
							        <li>Make the robot resilient to various lighting conditions and environments.</li>
							        <li>Ensure the robot ends gracefully or abandons its mission when encountering unsolvable edge cases, avoiding unwanted behavior.</li>
							        <li>Make the robot completely autonomous (once provided with a grid geometry, its point of origin, and its destination).</li>
							        <li>Integrate all the above points to create the LineZoomer.</li>
							        <li>Learn more about embedded systems and have fun!</li>
							    </ul>
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Design</h2> <!--// put your heading here -->
								<h2>Software Design</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>
									Our implementation of the LineZoomer works by line-following until it detects an intersection. It then refers to the turning commands generated by its algorithm to decide where it needs to turn at the intersection.
								</p>
								<p>
									<strong>Pathfinding:</strong><br>
									Our software starts by generating a rectangular graph of dimensions <em>n</em> x <em>m</em>. This graph is represented by its adjacency list, in other words, as a (<em>nm</em>, <em>nm</em>) matrix <em>G</em>, where the coefficient <em>G<sub>i,j</sub></em> represents the distance of the directional path from node <em>i</em> to node <em>j</em>. In our project, the matrix is symmetrical, with all coefficients initially set to infinity (representing no path between nodes), except for <em>G<sub>i,i+1</sub></em>, <em>G<sub>i,i-1</sub></em>, <em>G<sub>i,i-m</sub></em>, and <em>G<sub>i,i+m</sub></em>, which are set to 1.
								</p>
								<p>
									We then select a starting point and a destination from the nodes in the graph. Dijkstra's algorithm is used to find the optimal path between these nodes.
								</p>
								<p>
									<strong>Dijkstra's Algorithm:</strong><br>
									Dijkstra's algorithm works as follows:
									<pre>
function Dijkstra(graph, source):
    distance = [infinity] * len(graph)
    distance[source] = 0
    priority_queue = [(0, source)]  # (distance, node)
    
    while priority_queue is not empty:
        current_distance, current_node = priority_queue.pop_min()
        
        if current_distance > distance[current_node]:
            continue  # Skip if already finalized
        
        for neighbor, weight in graph[current_node]:
            distance_via_current = current_distance + weight
            
            if distance_via_current < distance[neighbor]:
                distance[neighbor] = distance_via_current
                priority_queue.add((distance_via_current, neighbor))
    
    return distance
									</pre>
									We implemented the algorithm using a priority queue with a binary heap, resulting in a time complexity of O((<em>V</em> + <em>E</em>) log <em>V</em>), where <em>V</em> = <em>n</em> x <em>m</em> (number of vertices) and <em>E</em> = 2<em>n</em> x <em>m</em> (number of edges).
								</p>
								<p>
									If the robot encounters an obstacle on the optimal path, we update the distance value of the given edge to infinity in the graph representation, return to the previous node, and recalculate a new optimal path. This is done by rerunning Dijkstra's algorithm with the current node as the new starting point and the same destination.
								</p>
								<p>
									Our algorithm returns the path as a list of nodes the robot must follow to reach the destination. However, the robot requires directions such as "left," "right," "forward," and "backward." A conversion algorithm determines these directions by using the cardinal orientation of the next node ("North," "East," "South," "West") and the robot's current orientation, which is deduced from its prior turns and initial orientation.
								</p>
								<p>
									We also implemented a visualization tool that tracks the robot's movement, displaying its starting point, current position, destination, detected obstacles, and paths (both attempted and current).
								</p>
								<p>
									<strong>Line Following:</strong><br>
									Our robot uses two image-processing algorithms for line-following:
									<ul>
										<li>After processing the image, we identify black lines using steps such as cropping the image to focus on the relevant field of view, blurring to reduce noise, converting to grayscale, and applying a binary mask. This is followed by erosion and dilation to remove residual noise.</li>
										<li>We then fit a straight line to the detected band. If the robot is misaligned (the line is off-center or missing), it uses a "barycenter" approach, aiming toward the largest black region detected.</li>
										<li>When aligned, the robot maintains its trajectory by following the fitted line.</li>
									</ul>
								</p>
								<p>
									<strong>Intersection Detection:</strong><br>
									To detect intersections, we identify the contours of black shapes in the image and analyze their complexity (number of edges). We apply filtering based on aspect ratios to ignore noise (e.g., shadows or minor irregularities). A detected shape with more than five edges indicates an intersection. For instance:
									<ul>
										<li>6 edges: Corner</li>
										<li>8 edges: T-junction</li>
										<li>12 edges: Cross-intersection</li>
									</ul>
									When an intersection is detected, the robot flags this event and determines the next turn based on its pathfinding logic.
								</p>
							</div>

							<div class="yui-u first">
								<h2>Hardware Design</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
							    <p>Our robot uses several sensors to gather data and servos to interact with the world.</p>
							
							    <h3>Line Detection Hardware</h3>
							    <p>
							        For line detection, we use a Raspberry Pi Camera module mounted with rubber bands on two popsicle sticks. 
							        While the sticks are hot glued on, the rubber bands allow for the camera to be adjusted. 
							        We use folded sticky notes to aim the camera downward toward the grid.
							    </p>
							    <p>
							        Our line detection algorithm is defined above, but our robot only needs the detected line angle for line following. 
							        It angles back toward the line if the detected angle is larger than +/-30 degrees. If the robot is within this band, it moves forward slightly.
							    </p>
							    <p>
							        To prevent the robot from driving into the void when no line is spotted, we save the most recent turn direction. 
							        If no line is detected, the robot turns in the reverse direction until the line is found. 
							        This strategy helps during normal operation and when overshooting a 90-degree turn. 
							        In fact, recovery from overshooting is easier than from undershooting, so we intentionally overshoot 90-degree turns.
							    </p>
							    <p>
							        The robot moves in short bursts to avoid getting stuck "wiggling." Constant turning can cause it to miss the center repeatedly, 
							        as it turns faster than it processes line detection. A slight nudge forward during turns helps it realign and prevent wiggling.
							    </p>
							
							    <h3>Object Detection Hardware</h3>
							    <p>
							        For object detection, we use an ultrasonic sensor aimed forward, hanging below the main chassis. 
							        It is secured only by its connecting wires. Although not ideal, this works because it is lightweight and does not need precise aiming.
							    </p>
							    <p>
							        The sensor is called approximately once per second, and its output has priority in the FSM (Finite State Machine) 
							        to determine the robot's next stage.
							    </p>
							
							    <h3>Movement Hardware</h3>
							    <p>
							        For movement, we use two Parallax servos controlled via PWM, which can report their current angle via output PWM. 
							        We drilled new holes in the chassis to accommodate these servo mountings, leading to imperfect alignment. 
							        As a result, the robot tends to veer left when moving straight for extended periods.
							    </p>
							    <p>
							        To correct for this, we move in short bursts, allowing for error correction. 
							        The Hall effect sensors used to detect servo angles are slightly inaccurate, so shorter movements also mitigate long-run errors.
							    </p>
							    <p>
							        Instead of relying on time to determine 90-degree turns or travel distances, we use the Hall sensors’ rotation angle. 
							        This approach is more accurate, as timing depends on servo strength and surface material.
							    </p>
							
							    <h3>Communication Hardware</h3>
							    <p>
							        We use a PiTFT screen, initially planned to display and control the robot's state, but we ran out of time to implement this fully. 
							        Currently, it shows the LAN IP address for SSH access to the Raspberry Pi.
							    </p>
							
							    <h3>Power Hardware</h3>
							    <p>
							        The Raspberry Pi (along with the PiTFT, camera, and ultrasonic sensor) is powered by a 5V rechargeable battery pack. 
							        As the robot design evolved, we opted not to mount the battery pack, instead keeping it leashed with a long USB cable.
							    </p>
							    <p>
							        The servos require higher voltage and are powered using 4x AA batteries in series.
							    </p>
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Testing</h2> <!--// Put your heading here -->
							</div>
							<div class="yui-u">
								<p>
									We planned and executed a rigorous testing strategy during our project, ensuring that every component was fully functional before progressing to the next phase. 
								</p>
								<p>
									We began by testing individual components to verify their functionality and compatibility with the Raspberry Pi (RPi). Specifically, we tested our control over motor rotation and the receipt of PWM signals by the servomotor. Additionally, we ensured that the ultrasound sensor provided accurate and reliable signals.
								</p>
								<p>
									Once the robot was built and a minimum viable product (MVP) version of the code was written, we tested the robot on small grids and intersections. These initial tests highlighted challenges and informed improvements to our setup. To simplify testing, we aimed to use consistent lighting conditions, employing multiple light sources around the robot to reduce shadows. Additionally, we created grids using taped-together white sheets to ensure a smooth, uniform surface without textures or designs.
								</p>
								<p>
									Early tests revealed that smaller grids caused the intersection detection algorithm to activate too frequently, as the robot could often see the next intersection while still navigating the current node. This meant that, although the robot completed its task, it did not fully utilize its line-following capabilities. To address this, we designed larger grids for subsequent tests.
								</p>
								<p>
									We thoroughly tested each new feature added to our MVP, addressing issues such as low speed and guidance inaccuracies. Once we finalized the grid setup and achieved satisfactory robot performance, we conducted a comprehensive battery of tests. These tests included varying the starting and ending points, placing obstacles on the grid, and exploring edge cases. For example, we tested scenarios where all paths to the target node were blocked, prompting the robot to safely give up and stop, as well as cases where the starting and ending positions were the same.
								</p>
								<p>
									After successfully passing these tests, we expanded testing to different rooms with varied lighting conditions. We were pleasantly surprised by the resilience of our final code, which performed significantly better than the initial MVP. The robot succeeded in all reasonable lighting scenarios, even in rooms with directional lighting that created shadows in its line of sight. This was a remarkable improvement compared to our early tests, where we had to manually hold a phone lamp above the robot’s camera to eliminate shadows.
								</p>
							</div>
						</div> <!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Drawings</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
								</p> <br /> <p> This is an example image  </p> <!--// Put Content here -->
								<img src="images/incident.png" title="Ha Ha ... that's me" alt="This is a picture" width="420" height="200"/> </p>
								<p> Credits to XKCD </p>
							</div>
						</div><!--// Your section has ended -->
						
						<div class="yui-gf"> <!--// This is a section -->
						    <div class="yui-u first">
						        <h2>Limitations</h2> <!--// Put your heading here -->
						    </div>
						    <div class="yui-u">
						        <p>While we managed to fix most problems that appeared, there were some issues that we could not handle.</p>
						
						        <h3>Camera</h3>
						        <ul>
						            <li>The camera has a small FOV, and thus it can be difficult to acquire the line.</li>
						            <li>The robot does not handle small grids well, as it cannot see closer than ~5.5 in away from the axles. Grids of a smaller shape cannot be traversed properly.</li>
						        </ul>
						
						        <h3>Detection</h3>
						        <ul>
						            <li>The filters we used are resilient, but have issues when the environment has bad lighting. We managed to make it resilient enough to not need perfect lighting, but some issues still remain.</li>
						            <li>The filters require a dark line on a light grid.</li>
						            <li>The robot cannot detect chasms, only obstacles.</li>
						            <li>In certain scenarios when the robot rapidly switches between the barycenter and line center detection algorithms, wiggling may occur. The problem eventually resolves itself, as the robot is nudged forward on every wiggle, but it drastically slows down movement.</li>
						            <li>Lines which are not simple rectangles and are more complex (have more edges), can be falsely identified as intersections.</li>
						            <li>The robot needs flat terrain (with decent traction). Turning on varied terrain yields inconsistent end angles, and terrain shadows may interfere with detection.</li>
						        </ul>
						
						        <h3>Movement</h3>
						        <ul>
						            <li>While the robot can follow gently-curved paths and irregular graphs, it can only deal with intersections at 90 degrees.</li>
						            <li>The robot also has a center of rotation in the front, and thus requires more space to turn than would be expected, being another reason why small grids do not work.</li>
						            <li>The robot turns a little when going straight a long distance due to non-perfect wheel alignment. We mitigate this by not driving in a straight line for long periods of time.</li>
						            <li>When turns start at very specific angles which result in a destination angle near 360 (or 0) degrees, the instability of the Hall sensors can result in the robot not moving far enough (terminating move command early).</li>
						            <li>Applying any external rotation to the wheels during precise forward movement or precise turning can cause incorrect movement.</li>
						        </ul>
						
						        <h3>Ultrasonic Sensor</h3>
						        <ul>
						            <li>The ultrasonic sensor may not detect objects that would hit the top of the robot, due to its position on the lower half of the chassis.</li>
						        </ul>
						
						        <h3>Controls / Pathfinding</h3>
						        <ul>
						            <li>The robot has no controls on itself, and can only be controlled via SSH.</li>
						            <li>The robot is leashed to a power pack, which must be moved when the robot moves far enough away.</li>
						            <li>The edge case of having the start node be the same as the end node is unhandled.</li>
						        </ul>
						    </div>
						</div><!--// Your section has ended -->


						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Results</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
								</p> <!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Testing</h2> <!--// Put your heading here -->
							</div>
							<div class="yui-u">
								<p>
									We planned and executed a rigorous testing strategy during our project, ensuring that every component was fully functional before progressing to the next phase. 
								</p>
								<p>
									We began by testing individual components to verify their functionality and compatibility with the Raspberry Pi (RPi). Specifically, we tested our control over motor rotation and the receipt of PWM signals by the servomotor. Additionally, we ensured that the ultrasound sensor provided accurate and reliable signals.
								</p>
								<p>
									Once the robot was built and a minimum viable product (MVP) version of the code was written, we tested the robot on small grids and intersections. These initial tests highlighted challenges and informed improvements to our setup. To simplify testing, we aimed to use consistent lighting conditions, employing multiple light sources around the robot to reduce shadows. Additionally, we created grids using taped-together white sheets to ensure a smooth, uniform surface without textures or designs.
								</p>
								<p>
									Early tests revealed that smaller grids caused the intersection detection algorithm to activate too frequently, as the robot could often see the next intersection while still navigating the current node. This meant that, although the robot completed its task, it did not fully utilize its line-following capabilities. To address this, we designed larger grids for subsequent tests.
								</p>
								<p>
									We thoroughly tested each new feature added to our MVP, addressing issues such as low speed and guidance inaccuracies. Once we finalized the grid setup and achieved satisfactory robot performance, we conducted a comprehensive battery of tests. These tests included varying the starting and ending points, placing obstacles on the grid, and exploring edge cases. For example, we tested scenarios where all paths to the target node were blocked, prompting the robot to safely give up and stop, as well as cases where the starting and ending positions were the same.
								</p>
								<p>
									After successfully passing these tests, we expanded testing to different rooms with varied lighting conditions. We were pleasantly surprised by the resilience of our final code, which performed significantly better than the initial MVP. The robot succeeded in all reasonable lighting scenarios, even in rooms with directional lighting that created shadows in its line of sight. This was a remarkable improvement compared to our early tests, where we had to manually hold a phone lamp above the robot’s camera to eliminate shadows.
								</p>
							</div>
						</div> <!--// Your section has ended -->
						

						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Work Distribution</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p> Lawrence took care of the most of the hardware testing and setup as well as the actual robot's "brain", whilst Xavier handled all of the iamge processing, graph creation, optimal path calculation and translation as well as the step by step visualization of the robot's position. We both equally participated in the debugging and testing of each iteration of our robot, proposing out ideas on how to improve it even on parts that we didn't directly code. 									<!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->
						
						<div class="yui-gf"> <!--// This is a section -->
							<div class="yui-u first">
								<h2>Git hub link:</h2> <!--// put your heading here -->
							</div>
							<div class="yui-u">
								<p> All our code is open source, feel free to use it to tryan make your own robot our improve our current one! Here's a link to the github repo we created: 									<!--// Put Content here -->
							</div>
						</div><!--// Your section has ended -->

					<div class="yui-gf">
						<div class="yui-u first">
							<h2>Code Appendix</h2>
						</div>
							<div class="yui-u">
							<!-- This is a good idea. HTML generated using hilite.me -->
							<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td><pre style="margin: 0; line-height: 125%"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #333333">*</span>
 <span style="color: #333333">*</span> blink.c<span style="color: #333333">:</span>
 <span style="color: #333333">*</span>      blinks the first LED
 <span style="color: #333333">*</span>      Gordon Henderson, projects<span style="color: #FF0000; background-color: #FFAAAA">@</span>drogon.net
 <span style="color: #FF0000; background-color: #FFAAAA">*/</span>

<span style="color: #557799">#include &lt;stdio.h&gt;</span>
<span style="color: #557799">#include &lt;wiringPi.h&gt;</span>

<span style="color: #333399; font-weight: bold">int</span> main (<span style="color: #333399; font-weight: bold">void</span>)
{
  printf (<span style="background-color: #fff0f0">&quot;Raspberry Pi blink</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&quot;</span>) ;

  <span style="color: #008800; font-weight: bold">if</span> (wiringPiSetup () <span style="color: #333333">==</span> <span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">1</span>)
    <span style="color: #008800; font-weight: bold">return</span> <span style="color: #0000DD; font-weight: bold">1</span> ;

  pinMode (<span style="color: #0000DD; font-weight: bold">0</span>, OUTPUT) ;         <span style="color: #888888">// aka BCM_GPIO pin 17</span>

  <span style="color: #008800; font-weight: bold">for</span> (;;)
  {
    digitalWrite (<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">1</span>) ;       <span style="color: #888888">// On</span>
    delay (<span style="color: #0000DD; font-weight: bold">500</span>) ;               <span style="color: #888888">// mS</span>
    digitalWrite (<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">0</span>) ;       <span style="color: #888888">// Off</span>
    delay (<span style="color: #0000DD; font-weight: bold">500</span>) ;
  }
  <span style="color: #008800; font-weight: bold">return</span> <span style="color: #0000DD; font-weight: bold">0</span> ;
}
</pre></td></tr></table></div>

						</div>
					</div>
					</div>

					<div class="yui-gf">
						<div class="yui-u first">
							<h2>Contact</h2>
						</div>
						<div class="yui-u">
						<p> Your good names. Probably Acknowledgments and Thanks </p>
						</div>
					</div>
					</div>

				</div><!--// .yui-b -->
			</div><!--// yui-main -->
		</div><!--// bd -->

		<div id="ft">
			<p><a href="http://validator.w3.org/check?uri=referer"><img src=
			"images/w3chatered.gif" title="W3C+Hates+Me" alt="W3C+Hates+Me"
			width="80" height="15" /></a>
			<a href="http://jigsaw.w3.org/css-validator/check/referer"><img src=
			"images/css_copy.gif" title="Valid+CSS%21" alt="Valid+CSS%21"
			width="80" height="15" /></a>
			<img src="images/code.gif" title="Handcrafted with sweat and blood" alt="Handcrafted with sweat and blood" width="80" height="15" />
			<img src="images/anybrowser.gif" title="Run Any Browser Any OS" alt="Runs on Any Browser Any OS" width="80" height="15" /></p>
			<br />
		</div><!--// footer -->

	</div><!-- // inner -->


</div><!--// doc -->


</body>
</html>
